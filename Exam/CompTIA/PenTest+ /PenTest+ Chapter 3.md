# CompTIA® PenTest+® Certification For Dummies®, 2nd Edition

# Chapter 3: Information Gathering

Type of information collect on Org during Information Gathering Phase:

- Email Addresses and phone numbers of employees (For Social Engineering)
- Public IP addresses used by Org.
- Target systems that are up and running.
- Open ports on target systems.
- Software used on the target systems.
- Software is running in cloud or self hosted (local server on network)

#

## Looking at Information-Gathering Tools and Techniques

Divide task into 2 parts: Passive and Active Information Gathering.

- Passive: collecting public information from internet about the Org without incoking any kind of communication with the target systems. Do first. (OSINT)
- Active: polling target system to find out about the system that are up and running, open ports and software being used. (Scanning)


Crawling Websites: Phrase to describe process of using an automated tool to fetch each page in website, analyzes the page, follows any link the page refers to and then fetch those pages.

Scraping Websites: Phrase to describe process of using program or bot to extract a copy of content in website.

Manual inspection of Web links: right clicking the link and choosing inspect from the context menu, Window opens display source code used to create the link and CSS code used.

Robots.txt: A file can be placed in the root folder of the site and contains rules on how the site and its pages are to be crawled. 
- You could create a rule in the robots.txt file that disallow specific crawling application from crawling the site.

#

## Google Hacking

Google hacking: Information gathering technique which specific keywords are used to search Google or other search engines (Bing) for information on the Internet.

- `site: <website> <keyword>`: The site keyword is used to search a specific website for a keyword.
  - If you are performing security test for Wiley publishing company: you could use site: www.wiley.com password to locate the login pages on the Wiley website.
  - This could be useful if you wanted to test Wiley’s login pages against SQL injection attacks.
 
- `intitle: <keyword>`: You can use intitle keyword to search the title of a page for specific keywords.
  - If you want to find web pages that contain the word “intranet” in the title, you could use intitle: intranet.
 
- `inurl: <keyword>`: The inurl operator will search the keyword given in the URLs found in the Google database.
  - If you want to locate sites that have the word “intranet” in the URL, you could use inurl: intranet.
 
- `intext: <keyword>`: The intext operator searches a web page for specific text.
  - If you want to search my company site for pages that contain the word “video,” you could use site: dcatt.ca intext: video.
 
- `filetype: <extension>`: One of my personal favorites is the filetype operator, which you can use to find results containing a specific file type.
  - You could search the Internet for sample penetration reports by filetype: pdf penetration test report.

#

## Referencing online Cybersecurity Sources

![image](https://github.com/user-attachments/assets/ebde4933-1e24-4758-8c1f-4fedbe7d9f02)

#

## Types of Data

Password dumps: You can use tools to obtain password dumps that display usernames and password hashes for each username. 
- The username list can be fed into a dictionary attack tool or you could use a password cracker to crack the password hashes.

File metadata: You can look at the file metadata on documents downloaded from the company’s website or other sources. 
- Metadata is additional information about the file such as the program or device used to create the file, the creator of the file, and location information.

Strategic search engine analysis/enumeration: You can use specific keywords in Google to target your search and find specific data about your target.

Website archive/caching: You can view older versions of the company’s website to get additional contact information or other information that could help in an attack. 
- For example, you could use www.archive.org and search for a website to view past versions of it.

Public source-code repositories: An archive of application source code that is made available to the public. 
- The repository may contain additional information with the source code such as technical documentation and code snippets that can be used to learn more about the company’s environment.

#

## Passive Information Gathering Tools

WHOIS: Database search tool used to discover Domain Name Information and IP Addresses about Org.
- www.godaddy.com/whois
- www.networksolutions.com/whois
- www.arin.net/whois.
  - Collect: Org's name, DNS server hosting the DNS data, contact info (email, phone number of company employees)

theHarvester: Program in Kali Linux, use to perform passive information gathering to collect info such as employee names, email address and subdomains, discover hosts owned by the Org. 
- Use it to collect public information from Google, LinkedIn, Twitter, Bing.
- Command: `theharvester -d wiley.com -b linkedin`: Searches LinkedIn users for Wiley.
- Command: `theharvester -d wiley.com -b all -l 100`: Collect info from all source "Google, LinkedIn, Twitter, limiting the result to 100.

SHODAN: search engine collects info about system connect to the internet (IoT devices).
- List of target company's publicly available servers and devices along with IP addresses, service running, open ports
- Viewing detail of the system; get list of vulnerabilities for that system.
- Map view shows phyical location of those servers.

MALTEGO: OSINT software shows a graphical representation of relationship between people, groups, web pages, and domains by analyzing online resources on (Facebook, Twitter, DNS and Whois info)
- Can create a graphic and add website address to the graphic then use Maltego to search for additional information such as Whois Information, phone number, location, email address associated with that website and have them added to the graph.

RECON-NG: OSINT tool built into Kali Linux, allows you to retrieve information such as contact names, email, DNS info, IP addresses.
- Not as easy as theHarvester; uses module concept similar to Metasploit Framework (PenTesting platform based on Ruby).

![image](https://github.com/user-attachments/assets/8f10aa24-c9c6-4424-86f2-89691775a234)

![image](https://github.com/user-attachments/assets/d0c857a0-581f-474d-97f9-8129a6cccd54)

CENSYS: Browser-based search engine that identifies hosts on Internet for Org.
- Identify the service, open ports on those system
- www.censys.io

![image](https://github.com/user-attachments/assets/36856aab-f116-4c3e-8ee9-debfb33a7f41)

FOCA: Fingerprinting Organizations with Collected Archives 
- Tool used to scan documents to collect metadata hidden from user.
- M.S Office file, Open Office files, PDF files can be scanned by FOCA to extract metadata.

DNS LOOKUPS/PROFILING: sending queries to DNS server to retrieve info on systems that might exist within the company (Mail server/Web server)
- This is the next step of whois lookup, to see what DNS record exist.
- nslookup: TCP/IP command in Windows and Linux that allows you to query DNS for different type of DNS records.
- dig: Linux command allows you to query DNS servers and obtain different records.

![image](https://github.com/user-attachments/assets/60bea1e0-8149-4fb8-8507-63701b3449df)
![image](https://github.com/user-attachments/assets/9ec094f7-7907-4351-9ba0-65cc1f5f9205)
![image](https://github.com/user-attachments/assets/96cb01a1-97f4-4c0a-b791-3b93c479e3f0)

Dig: Domain Information Gopher; Linux command
- dig www.wiley.com

![image](https://github.com/user-attachments/assets/dbe1ff3b-d32a-488e-b85b-bcd26ff4ee95)

- Can ask for a short version by adding `+short`
![image](https://github.com/user-attachments/assets/3123932c-e99f-4654-8b5f-9f746e6f2ef7)

![image](https://github.com/user-attachments/assets/e2b84a34-0db8-475b-ac39-600fce6a2e50)

- `dig wiley.com axfr` = zone transfer with dig; this probably will say "Transfer Failed" since company block full zone transfers from exposing too much info.

#

## Active Information Gathering/active Reconnaissance

Wardriving: Using a wireless scanner to discover wireless networks that exist within the company.

Network traffic: Capturing (also known as sniffing) and analyzing network traffic using a packet analyzer to discover sensitive information traveling on the network. You can also capture API requests and responses to see what type of calls a piece of software is making and the information submitted with the request or received in a response.

Cloud asset discovery: Collecting information to identify assets the company has across all cloud providers.

Third-party hosted services: Identifying any services the company is hosting with third-party companies.

Detection avoidance: Avoiding detection with the target’s intrusion detection systems while performing active reconnaissance. You will learn about some ways to do this with Nmap later in this chapter.

#

## Understanding Scanning and Enumeration

Two type of scanning: passive and active scanning.
- Passive scanning: do not interact with target hosts, but are capturing traffic on target network to see what you can pick up.
- Active scanning: Actually sending packets to the target systems to find out (OS that is running, services running on the OS).
- Perform passive scanning first because it's less likely to be detected.

#

## Passive Scanning

Involves monitoring network traffic for discovery

Packet Inspection: 
- look out for source and destination IP addresses to understand hosts existed on the network.
- look for layer 2 MAC addresses in the packets, there might be a spoof one.
- look for sensitive information in the paylod of the packets (usernames, passwords, confidential info).

Eavesdropping: Sniffing/ packet sniffing.
- Capturing network packets so you can analyze the traffic info that can help you exploit the network and systems.
- Tools: Wireshark, Linux: netdiscover (monitors network traffic for ARP message then use that info to help you discover IP addresses of hosts on newtwork and their MAC addresses).

#

## Active Scanning



#

## For Exam

- Tool: `wget` in linux to copy the contents of a website to local folder on your system so you can review contents offline.
- Even HTTPS is used; should inspect Secure Sockets Layer (SSL) certificates for flaws such as expiration dates and certificates that have been revoked/no longer valid.
- Whois, theHarvester, Maltego, Recon-ng, Censys are tools used for OSINT gathering.
- dig and nslookup: tools used to perform DNS profiling to help identify hosts that exist within an Org.
